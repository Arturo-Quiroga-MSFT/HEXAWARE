{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a6a9ba-90a1-4808-b8b7-584cf8371b0b",
   "metadata": {},
   "source": [
    "# Deep Research tool with Azure AI Foundry (preview)\n",
    "\n",
    "<img src=\"https://learn.microsoft.com/en-us/azure/ai-services/agents/media/agent-service-the-glue.png\" width=800>\n",
    "\n",
    "The **Deep Research tool** in the Azure AI Foundry Agent Service enables you to integrate a web-based research capability into your systems. The Deep Research capability is a specialized AI capability designed to perform in-depth, multi-step research using data from the public web.\n",
    "\n",
    "> https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/deep-research\n",
    "\n",
    "The **o3-deep-research model** and the GPT model deployments should be part of your AI Foundry project resulting in all three resources in the same Azure subscription and same region. Supported regions are **West US and Norway East.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47f876-0262-4406-8c7b-8bcb671720cd",
   "metadata": {},
   "source": [
    "At its core, **Deep Research** leverages a combination of OpenAI and Microsoft technologies, including **o3-deep-research**, various GPT models, and **Bing Search Grounding**, when integrated into an agent.\n",
    "\n",
    "- When an agent with Deep Research integration receives a research request — whether from a user or another application — it utilizes GPT-4o and GPT-4.1 to interpret the intent, fill in any missing details, and define a clear, actionable scope for the task.\n",
    "- Once the task is defined, the agent activates the Bing-powered grounding tool to gather a refined selection of recent, high-quality web content.\n",
    "- Following this, the o3-deep-research agent begins the research process by reasoning through the collected information. Rather than merely summarizing content, it evaluates, adapts, and synthesizes insights from multiple sources, adjusting its approach as new data emerges.\n",
    "- The entire process culminates in a structured report that not only provides the answer but also includes the model’s reasoning path, source citations, and any clarifications requested during the session, as explained by Arenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcdfeb20-fc2a-4c45-8c84-72b346f34fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --pre azure-ai-projects\n",
    "#%pip install pypandoc\n",
    "#%pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afb3437-0bc5-481c-bf34-109d466c99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pypandoc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from azure.ai.agents import AgentsClient\n",
    "from azure.ai.agents.models import DeepResearchTool, MessageRole, ThreadMessage\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from IPython.display import display, FileLink, Markdown\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fcc33e-1a1a-44e7-8e60-81c03ac44366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.13.5 (main, Jun 11 2025, 15:36:57) [Clang 17.0.0 (clang-1700.0.13.3)]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1265de15-5634-4f8c-9a5d-a56bd8277ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is 15-Jul-2025 11:35:17\n"
     ]
    }
   ],
   "source": [
    "print(f\"Today is {datetime.datetime.today().strftime('%d-%b-%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579f189-6729-44c0-9377-5eeeb9e6d71a",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac764c-9074-493b-8050-7f3bf2eb4cb1",
   "metadata": {},
   "source": [
    "> Only available now in \"West US\" and \"Norway East\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bdbc2a-b494-4777-956d-bc531e585b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foundry project\n",
    "project_endpoint = \"https://foundry-norway-east.services.ai.azure.com/api/projects/foundry-project-norway-east\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9f6e44-09d1-4b82-adb1-bb4f405969bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4.1\"  # a generic gpt model\n",
    "deep_research_model = \"o3-deep-research\"  # the new o3 deep research model deployed in your AI Foundry\n",
    "\n",
    "bingservice = \"aqbinggrounding002\"  # The Bing connection service in your AI foundry project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4de748d-439e-4aea-9378-5fd1ab34edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"documents\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd18e4a4-5ca2-4830-b101-b1c8e962cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.today().strftime('%d%b%Y_%H%M%S')\n",
    "\n",
    "md_results_file = os.path.join(RESULTS_DIR, f\"deep_research_results_{now}.md\")  # The name of the markdown output file\n",
    "docx_file = os.path.join(RESULTS_DIR, f\"deep_research_results_{now}.docx\")  # .docx outpyt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e2ddc-128b-46c6-ab8d-7106c0d0633f",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8931e94f-24c6-4704-940f-c36b88faa7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_print_new_agent_response(\n",
    "    thread_id: str,\n",
    "    agents_client: AgentsClient,\n",
    "    last_message_id: Optional[str] = None,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetches and prints the latest response from an agent in a given thread.\n",
    "\n",
    "    Args:\n",
    "        thread_id (str): The ID of the thread to fetch the agent's response from.\n",
    "        agents_client (AgentsClient): The client used to interact with the agents service.\n",
    "        last_message_id (Optional[str], optional): The ID of the last message that was processed. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The ID of the latest message if there is new content, otherwise returns the last_message_id.\n",
    "    \"\"\"\n",
    "    response = agents_client.messages.get_last_message_by_role(\n",
    "        thread_id=thread_id,\n",
    "        role=MessageRole.AGENT,\n",
    "    )\n",
    "    if not response or response.id == last_message_id:\n",
    "        return last_message_id  # No new content\n",
    "\n",
    "    print(\"\\nAgent response:\")\n",
    "    print(\"\\n\".join(t.text.value for t in response.text_messages))\n",
    "\n",
    "    for ann in response.url_citation_annotations:\n",
    "        print(\n",
    "            f\"URL Citation: [{ann.url_citation.title}]({ann.url_citation.url})\"\n",
    "        )\n",
    "\n",
    "    return response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa53624-d373-4755-a2e0-bb0767cbf734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_research_summary(message: ThreadMessage,\n",
    "                            filepath: str = md_results_file) -> None:\n",
    "    \"\"\"\n",
    "    Creates a research summary from the provided message and writes it to a file.\n",
    "\n",
    "    Args:\n",
    "        message (ThreadMessage): The message containing the content for the research summary.\n",
    "        filepath (str, optional): The path to the file where the research summary will be written in a markdown format.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not message:\n",
    "        print(\"Error: No message content provided\")\n",
    "        return\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as fp:\n",
    "        # Write text summary\n",
    "        text_summary = \"\\n\\n\".join(\n",
    "            [t.text.value.strip() for t in message.text_messages])\n",
    "        fp.write(text_summary)\n",
    "\n",
    "        # Write unique URL citations, if present\n",
    "        if message.url_citation_annotations:\n",
    "            fp.write(\"\\n\\n## References\\n\")\n",
    "            seen_urls = set()\n",
    "            for ann in message.url_citation_annotations:\n",
    "                url = ann.url_citation.url\n",
    "                title = ann.url_citation.title or url\n",
    "                if url not in seen_urls:\n",
    "                    fp.write(f\"- [{title}]({url})\\n\")\n",
    "                    seen_urls.add(url)\n",
    "\n",
    "    print(f\"Research summary written to '{filepath}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe977e-98c3-4e6c-818b-909ae62f105a",
   "metadata": {},
   "source": [
    "## Project & tool definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1c5bec5-e5c7-4922-ae0b-794a00525b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")\n",
    "\n",
    "conn_id = project_client.connections.get(name=bingservice).id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe15af61-7b30-4f7e-8cb6-3d85d981e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_research_tool = DeepResearchTool(\n",
    "    bing_grounding_connection_id=conn_id,\n",
    "    deep_research_model=deep_research_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d700b-703f-42e5-823a-e7f9e45c9b5b",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b6de670-5843-4025-b0cf-ababd4508549",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me the latest research into magentic-ui from Microsoft's autogen over the last year. Do not ask questions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d955d8f1-920c-481d-bfd4-c8315ba74c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Created agent, ID: asst_qR2G6ZR1f4CWHN5FHeAUER6c\n",
      "🧵 Created thread, ID: thread_0tOPCGBVLfxhgkx21YJhZKqI\n",
      "📩 Created message, ID: msg_Qrjvkt5etXhxEdaBrSKByBTj\n",
      "⏳ Start processing the message... this may take a few minutes to finish. Be patient!\n",
      "\n",
      "Agent response:\n",
      "I'll gather the most recent research from the last year related to 'magentic-ui' within Microsoft's AutoGen, including advancements, publications, and main findings. I'll share a structured summary with clear sources.\n",
      "\n",
      "Title: Latest Research on Magentic-UI in Microsoft AutoGen\n",
      "\n",
      " Starting deep research... \n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Clarifying terminology**\n",
      "\n",
      "I’m digging into 'magentic-ui' from Microsoft's AutoGen. It could be an internal tool or project, possibly involving Automated Generation UI. I'll search to confirm. 【1†Bing Search】\n",
      "\n",
      "URL Citation: [Bing Search: 'Magentic UI Microsoft AutoGen'](https://www.bing.com/search?q=Magentic%20UI%20Microsoft%20AutoGen)\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Gathering sources**\n",
      "\n",
      "I’m pulling together official documentation, academic papers, and reputable sources on Magentic-UI, a human-centered web agent. This involves examining GitHub repositories, Microsoft Research blog posts, and other trustworthy publications.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Introducing Magentic-UI**\n",
      "\n",
      "This blog post introduces Magentic-UI, an open-source human-centered agent for real-time collaborative web-based tasks, focusing on transparency, controllability, and safety.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: \n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Evaluating the GitHub repository**\n",
      "\n",
      "I’m analyzing the GitHub repository for Magentic-UI, noting its team of specialized agents, improvements over AutoGen's MultimodalWebSurfer, and the Orchestrator's task delegation capabilities.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Understanding system components**\n",
      "\n",
      "I’m examining the core elements of Magentic-One, such as the orchestrator and various agents like file surfer and websurfer. The system's performance enhancements and integration details are also mentioned.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Spotlighting the future**\n",
      "\n",
      "Introducing Magentic-One, a groundbreaking multi-agent system, showcasing advancements in agentic AI for open-ended tasks and emphasizing practical implementation.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Mapping out the timeline**\n",
      "\n",
      "The timeline includes the introduction of Magentic-One (Nov 2024) and Magentic-UI (May 2025), both pivotal in the AutoGen ecosystem.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Determining next steps**\n",
      "\n",
      "I’m figuring out how to access a hidden link on \"Read the technical report\" by considering page source, clicking the link directly, or employing workarounds like using unique search terms or `browser.open`. 【1†Bing Search】\n",
      "\n",
      "URL Citation: [Bing Search: '\"Magentic-One\" \"technical report\"'](https://www.bing.com/search?q=%22Magentic-One%22%20%22technical%20report%22)\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Confirming arXiv listing**  \n",
      "I'm verifying if \"Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks\" is indeed an arXiv entry, and checking for any publication or update dates.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Highlighting advancements**\n",
      "\n",
      "I’m mapping out the key features of Magentic-UI, including co-planning, co-tasking, and action guards, all built on the Magentic-One framework. Notably, the integration with Azure AI enhances human oversight and interactive plan learning, adding to the system's capabilities.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Mapping out updates**\n",
      "\n",
      "I’m gathering info on Magentic-UI, outlining its core features, underlying technologies, and notable advancements for a comprehensive report.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Checking Data Sources**\n",
      "\n",
      "I'm thinking about the accuracy and origins of numeric data in MarkTechPost's blog, considering sources like a Microsoft Research blog or an internal test.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary:  【1†Bing Search】\n",
      "\n",
      "URL Citation: [Bing Search: '\"71% (from 30.3% to 51.9%)\"'](https://www.bing.com/search?q=%2271%25%20%28from%2030.3%25%20to%2051.9%25%29%22)\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Gathering insights**\n",
      "\n",
      "I'm gathering info on \"71%\" and \"task completion 51.9%\" by searching within MarkTechpost to see if they reference their sources.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Analyzing MarkTechPost**\n",
      "\n",
      "MarkTechPost's insights include 71% task completion improvement, 10% help requests, co-planning UI, modular agents, plan reuse, Docker safety, action guards, red-team evaluation, and integration with Azure AI Foundry.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Taking a closer look**\n",
      "\n",
      "Bringing together details on Magentic-UI, including improvements, features, and performance benchmarks. Progressing towards a cohesive structure with clear references and images. 【1†Bing Search】\n",
      "\n",
      "URL Citation: [Bing Search: 'Magentic-UI screenshot browser UI'](https://www.bing.com/search?q=Magentic-UI%20screenshot%20browser%20UI)\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Navigating options**\n",
      "\n",
      "I’m thinking through various sources like WindowsClub, PyPI, and news outlets like BetaNews to find images or screenshots. Each source might offer unique perspectives on the topic.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Evaluating sources**\n",
      "\n",
      "I’m gathering info on BetaNews and MarkTechpost for GAIA test results and key features for accuracy.\n",
      "\n",
      "**Listing sources**\n",
      "\n",
      "I’m ensuring a comprehensive list of academic papers, blog posts, and official documentation for the last year, including GitHub and PyPI for open-source references. 【1†Bing Search】\n",
      "\n",
      "URL Citation: [Bing Search: 'Magentic-UI interface screenshot image Microsoft'](https://www.bing.com/search?q=Magentic-UI%20interface%20screenshot%20image%20Microsoft)\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Searching for imagery**\n",
      "\n",
      "I'm working on finding an image for a Microsoft blog post or BetaNews. Checking BetaNews' HTML for image tags or file extensions to identify relevant images.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Assessing content readiness**\n",
      "\n",
      "Checking the potential effect of incomplete content on \"find\" functionality, considering BetaNews' lazy loading or immediate image approach.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Pulling together sources**\n",
      "\n",
      "I'm gathering info on images from BetaNews or Microsoft's CDN, mentioning figures like Figure 1 in UI screenshots.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Evaluating options**\n",
      "\n",
      "I’m gathering info on The Windows Club article and GitHub readme for potential screenshots. Considering if images enhance the report or if we should proceed without them.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Illustrating steps**\n",
      "\n",
      "The article shows how to use an interface, likely with a figure or screenshot.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Examining source issues**  \n",
      "I’m thinking through how the Windows Club might not show images properly because they could be embedded within `<img src>` tags in scripts. This makes me curious about the root cause.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Describing the interface**\n",
      "\n",
      "I'm detailing the WindowsClub article's interface, mentioning the two-pane view: the left pane shows the agent's plan and progress, while the right pane displays a live browser view.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Mapping out Magentic-UI**\n",
      "\n",
      "I’m detailing the mapping of co-planning, co-tasking, action guards, plan learning, and parallel task execution, with an emphasis on reliable sources and avoiding excessive references.\n",
      "\n",
      "**Assessing performance gains**\n",
      "\n",
      "Hmm, that's interesting—tasks success rates skyrocketed from ~30% to ~52% with minimal help. MarkTechPost numbers suggest a significant 71% improvement when human feedback is included.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "cot_summary: **Evaluating benchmarks**\n",
      "\n",
      "Magentic-UI consistently achieves high success rates on various benchmarks.\n",
      "\n",
      "**Detailing modular design**\n",
      "\n",
      "I’m outlining Magentic-One's advanced modular design and how AutoGenBench is integrated for thorough agentic evaluation.\n",
      "\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "\n",
      "Agent response:\n",
      "Final Report:\n",
      "# Magentic-UI: A Human-Centered AI Web Agent on Microsoft AutoGen\n",
      "\n",
      "## Background: AutoGen Framework and Magentic-One  \n",
      "**AutoGen** is Microsoft’s open-source framework for building multi-agent AI systems. In late 2024, Microsoft Research introduced **Magentic-One**, a generalist multi-agent system built on AutoGen, designed to tackle complex, open-ended web and file-based tasks【33:1†source】,【33:1†source】. Magentic-One uses a lead **Orchestrator** agent that plans and coordinates several specialized agents (for web browsing, file handling, coding, etc.) to accomplish multi-step goals【33:1†source】,【33:2†source】. It achieved performance on par with state-of-the-art agentic systems across multiple benchmarks – for example, matching top methods on challenging web-interaction tasks like GAIA, AssistantBench, and WebArena – all without custom-training new skills【33:3†source】,【33:1†source】. The system’s modular design allows agents to be added or removed as needed without reworking the entire architecture, making it highly extensible【33:3†source】. Microsoft open-sourced Magentic-One on the AutoGen platform (MIT-licensed) and provided a **technical report** detailing its architecture and results【33:3†source】. To facilitate rigorous evaluation of such AI agents, the team also released **AutoGenBench**, a benchmarking tool with controls for repeated trials and isolated execution to minimize side-effects during testing【33:3†source】.\n",
      "\n",
      "## Introducing Magentic-UI: A Human-Centered Web Agent  \n",
      "Building on the Magentic-One foundation, Microsoft Research announced **Magentic-UI** in May 2025 as an experimental, human-centered AI web agent. Magentic-UI is an open-source prototype interface that collaborates with users in real time to automate complex web-based tasks while keeping the user **in control**, . Unlike fully autonomous agents that operate opaquely, Magentic-UI works *side-by-side* with the user in a web browser, offering transparency into its actions and plans . Users and the AI co-plan the task steps, and the system actively seeks user guidance or approval at decision points, rather than executing unchecked actions. This design addresses a key limitation of earlier agents: the lack of visibility into what the AI is doing and inability for users to intervene mid-task【33:4†source】,【33:4†source】. Magentic-UI is released under the MIT License and is available on GitHub, and it’s also integrated into Azure AI **Foundry** (an innovation hub for experimental AI), making it easy for developers and researchers to try out. In essence, Magentic-UI represents a shift from “autonomous” web automation towards a **human-in-the-loop** approach, prioritizing user oversight, safety, and collaboration【33:5†source】.\n",
      "\n",
      "## Key Features of Magentic-UI  \n",
      "Magentic-UI introduces several new features and interaction mechanisms to ensure a **collaborative and transparent** user experience:\n",
      "\n",
      "- **Co-Planning:** The user and agent collaboratively draft a step-by-step plan for the task before execution. Magentic-UI proposes an initial natural-language plan, which the user can review and modify using a plan editor (adding, removing, or editing steps). The plan must be approved by the user, ensuring the AI’s strategy aligns with the user’s intent before any action is taken. This up-front planning process may add a bit of initial overhead, but it often saves time by preventing mistakes and improves the chances of success.\n",
      "\n",
      "- **Co-Tasking:** The user remains in the loop during task execution. Magentic-UI provides real-time visibility into its actions and allows the user to **pause or intervene** at any point, . A live browser view is displayed alongside the plan, showing the web pages and interactions as the AI navigates them. The user can directly take control (e.g. click or type in the live browser panel) or give feedback via chat to guide the AI if it goes off track. The agent is also capable of asking the user for clarification or help when it encounters ambiguity or obstacles. This two-way collaboration ensures the AI doesn’t stray far from the user’s goals.\n",
      "\n",
      "- **Action Guards (Safety Approvals):** Magentic-UI never performs potentially risky or irreversible actions without asking the user first. It implements *action guards* that pause the agent whenever a sensitive step is reached – for instance, clicking a “Submit” or “Buy” button, deleting data, or navigating to an untrusted site. The user must explicitly approve these guarded actions before the agent continues. Users can configure how often approvals are required or define specific allow-lists for websites and actions【33:5†source】,. In other words, if Magentic-UI is about to do something significant like finalizing a purchase or closing a tab, it will **“ask first”** instead of acting unilaterally【33:5†source】. This mechanism gives users fine-grained control to prevent unwanted outcomes.\n",
      "\n",
      "- **Plan Learning and Reuse:** The system can **learn from experience** by saving successful task plans and reusing them in the future. After completing a task, Magentic-UI stores the verified plan in a **plan gallery**. When a similar task is requested later, it can retrieve a past plan (or parts of it) as a starting point instead of planning from scratch. This capability allows repeat or periodic tasks to be completed much faster on subsequent runs. In fact, reusing a previously learned plan can reduce the execution time or latency of a task by up to **3×** compared to generating a new plan from zero. Over time, Magentic-UI “gets better” at tasks the more it does them, since it refines its strategies and avoids past mistakes.\n",
      "\n",
      "- **Parallel Task Execution:** To boost productivity, Magentic-UI supports running multiple tasks in parallel within separate sessions. The interface includes a session manager (on the sidebar) where users can launch new task sessions or switch between ongoing tasks. Visual status indicators show each session’s state – for example, whether the agent is waiting for user input (🔴), actively working (↺), or finished (✅),. This way, a user can supervise several automated tasks concurrently. For instance, one session might be filling out a form while another scrapes data from a website, and the user can hop between them as needed. Parallel execution, combined with the session status alerts, ensures efficiency without losing oversight.\n",
      "\n",
      "## System Architecture and Technical Improvements  \n",
      "Magentic-UI is built on the same multi-agent architecture as Magentic-One, with a team of specialized AI agents coordinated by a central orchestrator. The major components of the system include:  \n",
      "\n",
      "- **Orchestrator:** The lead agent (powered by a large language model, e.g. GPT-4) that decomposes the user’s request into a plan and oversees its execution. The Orchestrator decides which steps to delegate to which agent (or back to the user) and monitors overall progress. It maintains a *Task Ledger* of facts, goals, and the current plan, and a *Progress Ledger* to track execution status【33:2†source】,【33:2†source】. If a step fails or conditions change, the Orchestrator can even revise the plan on the fly (with user permission) to recover from errors. This dynamic planning loop allows the system to adapt to unexpected situations while keeping the user informed.  \n",
      "\n",
      "- **WebSurfer:** An LLM-driven web-browsing agent that controls a real browser. It can navigate to URLs, click links or buttons, fill in text fields, scroll pages, and interact with web content as needed【33:2†source】. In Magentic-UI, the WebSurfer’s capabilities have been **significantly enhanced** compared to the original Magentic-One version. It can manage multiple browser tabs, select options from drop-down menus, handle file uploads, and even perform multimodal queries (interpreting text and images on a page). These improvements enable the agent to handle more complex web interfaces and tasks than before. The WebSurfer reports back the results of each action (e.g. the new page content or state) to the Orchestrator, which is displayed live in the UI.  \n",
      "\n",
      "- **Coder:** A coding agent specialized in writing and executing code. When a task requires data processing, computation, or automation beyond simple browsing, the Orchestrator calls on the Coder agent. The Coder can generate Python or shell scripts to perform subtasks (for example, parsing data or performing calculations). Importantly, Magentic-UI executes all code in a **secure Docker container**, isolating these operations from the user’s actual system. The Docker sandbox prevents any harmful commands from affecting the host machine and ensures that the agent cannot access the user’s local files or credentials unless explicitly allowed. After running the code, the Coder returns the results or output back to the Orchestrator.  \n",
      "\n",
      "- **FileSurfer:** A file-handling agent that can navigate directories and read or convert files. If the task involves analyzing a local document, searching within files, or extracting information, the FileSurfer agent is tasked with those steps【33:2†source】,. It uses tools (like the MarkItDown toolkit) to convert files into readable text (e.g. converting PDFs or images to Markdown) and can answer questions about file contents. Like the Coder, the FileSurfer operates within a sandboxed environment (via Docker) to ensure it only accesses permitted files. This agent enables Magentic-UI to handle tasks that blend web browsing with local data processing – for example, downloading a report from the web and then summarizing it.  \n",
      "\n",
      "- **UserProxy:** A special agent that represents the human user within the agent team. The UserProxy acts as a placeholder for the cases where part of the task is explicitly assigned back to the user. For instance, the Orchestrator may determine that a certain step (like deciding on a preference or providing a piece of personal information) is better handled by the user; it will delegate that step to the UserProxy agent, which essentially pauses and awaits the real user’s input,. In practice, this means Magentic-UI knows when to *ask* the user to do something rather than attempting it blindly. The inclusion of a UserProxy ensures that the human can be seamlessly integrated into the workflow as another “agent” when needed, under the Orchestrator’s coordination.\n",
      "\n",
      "**Interface and Controls:** The Magentic-UI front-end ties these agents together in a cohesive interface. The display is split into a **dual-panel view**: one side shows the agent’s plan, step-by-step progress, and any questions for the user, while the other side shows a live browser window under the WebSurfer’s control,. As the Orchestrator and agents work through the plan, every intermediate action and result is logged visibly. A progress bar at the top visualizes how close the task is to completion. Users interact through a chat prompt and the plan editor on the left panel, and they can also directly manipulate the live browser panel on the right if needed (e.g. to demonstrate an action). This transparent UI lets users literally *see* what the AI is doing at each moment, which builds trust and allows timely intervention if something looks off-track,.\n",
      "\n",
      "**Safety Mechanisms:** Magentic-UI’s design places a strong emphasis on safety and preventing unintended side-effects. All agent actions (web browsing and code execution) run within **sandboxed Docker containers**, providing a protective barrier between the AI agents and the user’s actual environment【33:5†source】. This means the AI cannot make system-level changes or access sensitive data on the host machine. For example, any script executed by the Coder agent runs in isolation, and any browser session run by WebSurfer is contained (with no saved cookies or credentials from the user unless provided). Additionally, users can define allowed domains or apply filters to constrain where the WebSurfer agent is permitted to navigate【33:5†source】. Combined with the earlier-mentioned *Action Guards* (which require manual confirmation for high-risk steps), these measures form a multi-layered defense. In fact, Microsoft conducted **red-team evaluations** on Magentic-UI’s agent system to probe its robustness against malicious scenarios. The tests included attempts at phishing (tricking the agent into revealing sensitive info or visiting harmful sites) and prompt injection attacks. The results were encouraging – Magentic-UI either sought user clarification or outright blocked the unsafe actions in those cases, rather than blindly executing them. This demonstrates that the system’s safeguards and human-in-the-loop checks can effectively mitigate many potential risks. Magentic-UI also runs by default with no persistent login credentials in the browser, ensuring that the agent cannot accidentally leverage the user’s authenticated sessions on websites. All these technical improvements (sandboxing, allow-lists, approval gates, etc.) are aimed at making the agent **transparent, secure, and controllable** in real-world use,.\n",
      "\n",
      "## Performance and Notable Findings  \n",
      "Over the past year, Microsoft has evaluated Magentic-UI’s performance both as an autonomous agent and with human-in-the-loop assistance. The underlying multi-agent system already proved capable on complex tasks: **Magentic-One** (without user input) demonstrated competitive results on several benchmarks of agent performance【33:1†source】. For example, it achieved strong success rates on the **GAIA** challenge (a test of general AI assistants requiring reasoning, tool use, and web interaction) and on **WebArena/WebGames** (interactive web browsing tasks), coming close to prior state-of-the-art systems【33:3†source】,. These results were attained with Magentic-One’s generalist architecture *without* needing specialized training or tuning for each task, highlighting the power of the modular multi-agent approach【33:1†source】.\n",
      "\n",
      "More interestingly, Magentic-UI’s human-centered features have led to **significant improvements** in task completion when even minimal user guidance is provided. In an experiment on the GAIA benchmark, allowing the agent to ask for occasional help or confirmation from a (simulated) user boosted the task success rate from about **30.3% to 51.9%**, a relative increase of **71%** in completion rate. In other words, by inserting a bit of human feedback into the loop, the agent solved many more tasks correctly than it could autonomously. Notably, Magentic-UI achieved this boost with only **light user involvement** – the system needed to request help in only roughly **10%** of the tasks, averaging just **1.1** user interactions per task in that experiment. This suggests that a small amount of timely human oversight can have a outsized positive impact on an agent’s reliability. As one report put it, *“in some scenarios, task completion went up by 71% just by including light human feedback”*【33:5†source】. These findings validate Magentic-UI’s design philosophy: rather than pursuing full autonomy at any cost, keeping a human in the loop for guidance and error correction dramatically improves outcomes on complex tasks while still maintaining efficiency.\n",
      "\n",
      "Another notable benefit observed is the impact of **plan reuse and learning** on efficiency. As mentioned, Magentic-UI can save successful workflows and retrieve them later. In tests, this **plan gallery** mechanism reduced the time to complete repeat tasks by up to threefold, since the agent could skip recalculating steps and immediately deploy a proven plan. For users, this means the agent becomes faster and more effective with each similar task it tackles – a form of cumulative learning. Additionally, Magentic-UI performed well even with **different AI models** powering it. The system is model-agnostic; while the default uses a version of GPT-4 for the Orchestrator, experiments have shown it can incorporate other language models to balance cost and performance【33:2†source】. This flexibility implies researchers or developers can plug in newer or domain-specific models into the agent roles to further improve certain capabilities (for instance, using a specialized vision model for the WebSurfer’s image analysis or a smaller, faster model for the Coder if real-time response is crucial)【33:2†source】.\n",
      "\n",
      "Crucially, the introduction of Magentic-UI did not reveal major new failure modes beyond those already present in agentic systems, and the added oversight actually helps catch issues. The transparent interface allowed testers to catch and correct agent mistakes on the fly, preventing the kind of hidden errors that fully-autonomous agents might carry through unnoticed. The internal evaluations, including the red-team tests described earlier, indicate that Magentic-UI’s layered safety approach is effective: the system **passed phishing and prompt-injection tests** by either refusing the malicious action or deferring to the user. This is a promising result for the safe deployment of such agents. Of course, Magentic-UI is not yet infallible – there were still tasks it could not complete and errors it could not overcome – but the overall trend shows that combining AI automation with human oversight yields a more robust and trustworthy performance.\n",
      "\n",
      "## Conclusion and Outlook  \n",
      "Magentic-UI represents a significant step forward in the evolution of “agentic” AI systems, demonstrating how **human-AI collaboration** can improve both the effectiveness and safety of AI assistants. By keeping the user in the loop, the system addresses long-standing concerns about transparency and loss of control in autonomous AI. Rather than aiming to replace the user, Magentic-UI is designed to **empower** the user – handling the busywork of web-based tasks while continually soliciting user guidance for critical decisions. Early research indicates that this approach can substantially boost task success rates (as seen with the 71% improvement on GAIA) without imposing a heavy burden on the user,. The open-source release of Magentic-UI (and its underlying Magentic-One framework) is also notable – it invites the broader research and developer community to experiment with the system, reproduce results, and build upon its human-centered design【33:5†source】,. Microsoft’s team has highlighted that Magentic-UI is still a prototype and **far from human-level performance** on many tasks, and there are open challenges to tackle around reliability and potential misuse【33:1†source】. As AI agents become more powerful, ensuring they remain **helpful, aligned, and safe** will be crucial【33:1†source】. Magentic-UI provides a valuable platform to explore these questions in a grounded way. \n",
      "\n",
      "In summary, over the past year Microsoft’s AutoGen project has evolved from the multi-agent capabilities of Magentic-One to the human-centric interface of Magentic-UI – adding transparency, user feedback loops, and safety features to create a more **trustworthy AI assistant for the web**. This research prototype has demonstrated encouraging results in combining the strengths of AI and human problem-solving. It lays a strong foundation for future intelligent assistants that can handle complex tasks *with* us rather than for us, ensuring that we as users maintain clarity and control. The work on Magentic-UI not only showcases new features and technical improvements in agent design, but also offers insights and tools (like AutoGenBench and the open-source codebase) for the wider AI community to advance the state of the art in cooperative, human-aligned AI systems. The coming years will likely build on these ideas, bringing us closer to AI agents that are both highly capable and **deeply collaborative** with their human partners. \n",
      "\n",
      "**Sources:** The information in this report is derived from Microsoft Research publications, technical documentation, and reputable AI industry analyses. Key references include the Microsoft Research blog announcements of *Magentic-One* (Nov 2024) and *Magentic-UI* (May 2025)【33:1†source】,, the Magentic-One technical report on arXiv【33:3†source】, the official Magentic-UI GitHub documentation,, and summaries by trusted tech outlets such as MarkTechPost and BetaNews that covered Magentic-UI’s features and experimental results,【33:5†source】. These sources provide detailed accounts of the system’s design, capabilities, and the latest research findings supporting the above summary.\n",
      "\n",
      "\n",
      "URL Citation: [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks ...](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n",
      "URL Citation: [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks ...](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n",
      "URL Citation: [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks ...](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks ...](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Microsoft Magentic-UI is an open source AI tool that lets humans stay ...](https://betanews.com/2025/05/19/microsoft-magentic-ui-is-an-open-source-ai-tool-that-lets-humans-stay-in-control/)\n",
      "URL Citation: [Microsoft Magentic-UI is an open source AI tool that lets humans stay ...](https://betanews.com/2025/05/19/microsoft-magentic-ui-is-an-open-source-ai-tool-that-lets-humans-stay-in-control/)\n",
      "URL Citation: [Using Magentic-UI, Microsoft’s Open-Source Human-centric web agent](https://www.thewindowsclub.com/use-microsofts-open-source-human-centric-web-agent-magentic-ui)\n",
      "URL Citation: [Using Magentic-UI, Microsoft’s Open-Source Human-centric web agent](https://www.thewindowsclub.com/use-microsofts-open-source-human-centric-web-agent-magentic-ui)\n",
      "URL Citation: [Using Magentic-UI, Microsoft’s Open-Source Human-centric web agent](https://www.thewindowsclub.com/use-microsofts-open-source-human-centric-web-agent-magentic-ui)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Using Magentic-UI, Microsoft’s Open-Source Human-centric web agent](https://www.thewindowsclub.com/use-microsofts-open-source-human-centric-web-agent-magentic-ui)\n",
      "URL Citation: [Using Magentic-UI, Microsoft’s Open-Source Human-centric web agent](https://www.thewindowsclub.com/use-microsofts-open-source-human-centric-web-agent-magentic-ui)\n",
      "URL Citation: [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks ...](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks ...](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n",
      "URL Citation: [Using Magentic-UI, Microsoft’s Open-Source Human-centric web agent](https://www.thewindowsclub.com/use-microsofts-open-source-human-centric-web-agent-magentic-ui)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Using Magentic-UI, Microsoft’s Open-Source Human-centric web agent](https://www.thewindowsclub.com/use-microsofts-open-source-human-centric-web-agent-magentic-ui)\n",
      "URL Citation: [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks ...](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n",
      "URL Citation: [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks ...](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n",
      "URL Citation: [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks ...](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n",
      "URL Citation: [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
      "URL Citation: [Using Magentic-UI, Microsoft’s Open-Source Human-centric web agent](https://www.thewindowsclub.com/use-microsofts-open-source-human-centric-web-agent-magentic-ui)\n",
      "🔄 Run status: RunStatus.IN_PROGRESS\n",
      "🔄 Run status: RunStatus.COMPLETED\n",
      "✅ Run finished with status: RunStatus.COMPLETED, ID: run_O7JwwSfUGOyCE0QTxQGRd5zD\n",
      "Research summary written to 'documents/deep_research_results_15Jul2025_113517.md'.\n",
      "\n",
      "Elapsed time = 10 minutes and 50 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with project_client:\n",
    "    with project_client.agents as agents_client:\n",
    "        agent = agents_client.create_agent(\n",
    "            model=model,\n",
    "            name=\"deep-research-agent\",\n",
    "            instructions=\n",
    "            \"You are a helpful agent that assists in researching scientific topics.\",\n",
    "            tools=deep_research_tool.definitions,\n",
    "        )\n",
    "\n",
    "        print(f\"🎉 Created agent, ID: {agent.id}\")\n",
    "\n",
    "        thread = agents_client.threads.create()\n",
    "        print(f\"🧵 Created thread, ID: {thread.id}\")\n",
    "\n",
    "        # Create message to thread\n",
    "        message = agents_client.messages.create(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=(prompt),\n",
    "        )\n",
    "        print(f\"📩 Created message, ID: {message.id}\")\n",
    "        print(\n",
    "            f\"⏳ Start processing the message... this may take a few minutes to finish. Be patient!\"\n",
    "        )\n",
    "\n",
    "        run = agents_client.runs.create(thread_id=thread.id, agent_id=agent.id)\n",
    "        last_message_id = None\n",
    "\n",
    "        while run.status in (\"queued\", \"in_progress\"):\n",
    "            time.sleep(1)\n",
    "            run = agents_client.runs.get(thread_id=thread.id, run_id=run.id)\n",
    "\n",
    "            last_message_id = fetch_and_print_new_agent_response(\n",
    "                thread_id=thread.id,\n",
    "                agents_client=agents_client,\n",
    "                last_message_id=last_message_id,\n",
    "            )\n",
    "            print(f\"🔄 Run status: {run.status}\")\n",
    "\n",
    "        print(f\"✅ Run finished with status: {run.status}, ID: {run.id}\")\n",
    "\n",
    "        if run.status == \"failed\":\n",
    "            print(f\"❌ Run failed: {run.last_error}\")\n",
    "\n",
    "        final_message = agents_client.messages.get_last_message_by_role(\n",
    "            thread_id=thread.id, role=MessageRole.AGENT)\n",
    "        if final_message:\n",
    "            create_research_summary(final_message)\n",
    "\n",
    "        # Clean-up and delete the agent once the run is finished.\n",
    "        #agents_client.delete_agent(agent.id)\n",
    "        #print(\"🗑️ Deleted agent\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "minutes, seconds = divmod(elapsed, 60)\n",
    "print(f\"\\nElapsed time = {minutes:.0f} minutes and {seconds:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257758c-7d0d-43fa-bebd-effbebad9dc7",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1383d583-075c-441b-a089-a07e85958af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 arturoquiroga  staff    21K Jul 15 11:46 documents/deep_research_results_15Jul2025_113517.md\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $md_results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3629fe2-f850-43c8-a9fb-94666860c702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Final Report:\n",
       "# Magentic-UI: A Human-Centered AI Web Agent on Microsoft AutoGen\n",
       "\n",
       "## Background: AutoGen Framework and Magentic-One  \n",
       "**AutoGen** is Microsoft’s open-source framework for building multi-agent AI systems. In late 2024, Microsoft Research introduced **Magentic-One**, a generalist multi-agent system built on AutoGen, designed to tackle complex, open-ended web and file-based tasks【33:1†source】,【33:1†source】. Magentic-One uses a lead **Orchestrator** agent that plans and coordinates several specialized agents (for web browsing, file handling, coding, etc.) to accomplish multi-step goals【33:1†source】,【33:2†source】. It achieved performance on par with state-of-the-art agentic systems across multiple benchmarks – for example, matching top methods on challenging web-interaction tasks like GAIA, AssistantBench, and WebArena – all without custom-training new skills【33:3†source】,【33:1†source】. The system’s modular design allows agents to be added or removed as needed without reworking the entire architecture, making it highly extensible【33:3†source】. Microsoft open-sourced Magentic-One on the AutoGen platform (MIT-licensed) and provided a **technical report** detailing its architecture and results【33:3†source】. To facilitate rigorous evaluation of such AI agents, the team also released **AutoGenBench**, a benchmarking tool with controls for repeated trials and isolated execution to minimize side-effects during testing【33:3†source】.\n",
       "\n",
       "## Introducing Magentic-UI: A Human-Centered Web Agent  \n",
       "Building on the Magentic-One foundation, Microsoft Research announced **Magentic-UI** in May 2025 as an experimental, human-centered AI web agent. Magentic-UI is an open-source prototype interface that collaborates with users in real time to automate complex web-based tasks while keeping the user **in control**, . Unlike fully autonomous agents that operate opaquely, Magentic-UI works *side-by-side* with the user in a web browser, offering transparency into its actions and plans . Users and the AI co-plan the task steps, and the system actively seeks user guidance or approval at decision points, rather than executing unchecked actions. This design addresses a key limitation of earlier agents: the lack of visibility into what the AI is doing and inability for users to intervene mid-task【33:4†source】,【33:4†source】. Magentic-UI is released under the MIT License and is available on GitHub, and it’s also integrated into Azure AI **Foundry** (an innovation hub for experimental AI), making it easy for developers and researchers to try out. In essence, Magentic-UI represents a shift from “autonomous” web automation towards a **human-in-the-loop** approach, prioritizing user oversight, safety, and collaboration【33:5†source】.\n",
       "\n",
       "## Key Features of Magentic-UI  \n",
       "Magentic-UI introduces several new features and interaction mechanisms to ensure a **collaborative and transparent** user experience:\n",
       "\n",
       "- **Co-Planning:** The user and agent collaboratively draft a step-by-step plan for the task before execution. Magentic-UI proposes an initial natural-language plan, which the user can review and modify using a plan editor (adding, removing, or editing steps). The plan must be approved by the user, ensuring the AI’s strategy aligns with the user’s intent before any action is taken. This up-front planning process may add a bit of initial overhead, but it often saves time by preventing mistakes and improves the chances of success.\n",
       "\n",
       "- **Co-Tasking:** The user remains in the loop during task execution. Magentic-UI provides real-time visibility into its actions and allows the user to **pause or intervene** at any point, . A live browser view is displayed alongside the plan, showing the web pages and interactions as the AI navigates them. The user can directly take control (e.g. click or type in the live browser panel) or give feedback via chat to guide the AI if it goes off track. The agent is also capable of asking the user for clarification or help when it encounters ambiguity or obstacles. This two-way collaboration ensures the AI doesn’t stray far from the user’s goals.\n",
       "\n",
       "- **Action Guards (Safety Approvals):** Magentic-UI never performs potentially risky or irreversible actions without asking the user first. It implements *action guards* that pause the agent whenever a sensitive step is reached – for instance, clicking a “Submit” or “Buy” button, deleting data, or navigating to an untrusted site. The user must explicitly approve these guarded actions before the agent continues. Users can configure how often approvals are required or define specific allow-lists for websites and actions【33:5†source】,. In other words, if Magentic-UI is about to do something significant like finalizing a purchase or closing a tab, it will **“ask first”** instead of acting unilaterally【33:5†source】. This mechanism gives users fine-grained control to prevent unwanted outcomes.\n",
       "\n",
       "- **Plan Learning and Reuse:** The system can **learn from experience** by saving successful task plans and reusing them in the future. After completing a task, Magentic-UI stores the verified plan in a **plan gallery**. When a similar task is requested later, it can retrieve a past plan (or parts of it) as a starting point instead of planning from scratch. This capability allows repeat or periodic tasks to be completed much faster on subsequent runs. In fact, reusing a previously learned plan can reduce the execution time or latency of a task by up to **3×** compared to generating a new plan from zero. Over time, Magentic-UI “gets better” at tasks the more it does them, since it refines its strategies and avoids past mistakes.\n",
       "\n",
       "- **Parallel Task Execution:** To boost productivity, Magentic-UI supports running multiple tasks in parallel within separate sessions. The interface includes a session manager (on the sidebar) where users can launch new task sessions or switch between ongoing tasks. Visual status indicators show each session’s state – for example, whether the agent is waiting for user input (🔴), actively working (↺), or finished (✅),. This way, a user can supervise several automated tasks concurrently. For instance, one session might be filling out a form while another scrapes data from a website, and the user can hop between them as needed. Parallel execution, combined with the session status alerts, ensures efficiency without losing oversight.\n",
       "\n",
       "## System Architecture and Technical Improvements  \n",
       "Magentic-UI is built on the same multi-agent architecture as Magentic-One, with a team of specialized AI agents coordinated by a central orchestrator. The major components of the system include:  \n",
       "\n",
       "- **Orchestrator:** The lead agent (powered by a large language model, e.g. GPT-4) that decomposes the user’s request into a plan and oversees its execution. The Orchestrator decides which steps to delegate to which agent (or back to the user) and monitors overall progress. It maintains a *Task Ledger* of facts, goals, and the current plan, and a *Progress Ledger* to track execution status【33:2†source】,【33:2†source】. If a step fails or conditions change, the Orchestrator can even revise the plan on the fly (with user permission) to recover from errors. This dynamic planning loop allows the system to adapt to unexpected situations while keeping the user informed.  \n",
       "\n",
       "- **WebSurfer:** An LLM-driven web-browsing agent that controls a real browser. It can navigate to URLs, click links or buttons, fill in text fields, scroll pages, and interact with web content as needed【33:2†source】. In Magentic-UI, the WebSurfer’s capabilities have been **significantly enhanced** compared to the original Magentic-One version. It can manage multiple browser tabs, select options from drop-down menus, handle file uploads, and even perform multimodal queries (interpreting text and images on a page). These improvements enable the agent to handle more complex web interfaces and tasks than before. The WebSurfer reports back the results of each action (e.g. the new page content or state) to the Orchestrator, which is displayed live in the UI.  \n",
       "\n",
       "- **Coder:** A coding agent specialized in writing and executing code. When a task requires data processing, computation, or automation beyond simple browsing, the Orchestrator calls on the Coder agent. The Coder can generate Python or shell scripts to perform subtasks (for example, parsing data or performing calculations). Importantly, Magentic-UI executes all code in a **secure Docker container**, isolating these operations from the user’s actual system. The Docker sandbox prevents any harmful commands from affecting the host machine and ensures that the agent cannot access the user’s local files or credentials unless explicitly allowed. After running the code, the Coder returns the results or output back to the Orchestrator.  \n",
       "\n",
       "- **FileSurfer:** A file-handling agent that can navigate directories and read or convert files. If the task involves analyzing a local document, searching within files, or extracting information, the FileSurfer agent is tasked with those steps【33:2†source】,. It uses tools (like the MarkItDown toolkit) to convert files into readable text (e.g. converting PDFs or images to Markdown) and can answer questions about file contents. Like the Coder, the FileSurfer operates within a sandboxed environment (via Docker) to ensure it only accesses permitted files. This agent enables Magentic-UI to handle tasks that blend web browsing with local data processing – for example, downloading a report from the web and then summarizing it.  \n",
       "\n",
       "- **UserProxy:** A special agent that represents the human user within the agent team. The UserProxy acts as a placeholder for the cases where part of the task is explicitly assigned back to the user. For instance, the Orchestrator may determine that a certain step (like deciding on a preference or providing a piece of personal information) is better handled by the user; it will delegate that step to the UserProxy agent, which essentially pauses and awaits the real user’s input,. In practice, this means Magentic-UI knows when to *ask* the user to do something rather than attempting it blindly. The inclusion of a UserProxy ensures that the human can be seamlessly integrated into the workflow as another “agent” when needed, under the Orchestrator’s coordination.\n",
       "\n",
       "**Interface and Controls:** The Magentic-UI front-end ties these agents together in a cohesive interface. The display is split into a **dual-panel view**: one side shows the agent’s plan, step-by-step progress, and any questions for the user, while the other side shows a live browser window under the WebSurfer’s control,. As the Orchestrator and agents work through the plan, every intermediate action and result is logged visibly. A progress bar at the top visualizes how close the task is to completion. Users interact through a chat prompt and the plan editor on the left panel, and they can also directly manipulate the live browser panel on the right if needed (e.g. to demonstrate an action). This transparent UI lets users literally *see* what the AI is doing at each moment, which builds trust and allows timely intervention if something looks off-track,.\n",
       "\n",
       "**Safety Mechanisms:** Magentic-UI’s design places a strong emphasis on safety and preventing unintended side-effects. All agent actions (web browsing and code execution) run within **sandboxed Docker containers**, providing a protective barrier between the AI agents and the user’s actual environment【33:5†source】. This means the AI cannot make system-level changes or access sensitive data on the host machine. For example, any script executed by the Coder agent runs in isolation, and any browser session run by WebSurfer is contained (with no saved cookies or credentials from the user unless provided). Additionally, users can define allowed domains or apply filters to constrain where the WebSurfer agent is permitted to navigate【33:5†source】. Combined with the earlier-mentioned *Action Guards* (which require manual confirmation for high-risk steps), these measures form a multi-layered defense. In fact, Microsoft conducted **red-team evaluations** on Magentic-UI’s agent system to probe its robustness against malicious scenarios. The tests included attempts at phishing (tricking the agent into revealing sensitive info or visiting harmful sites) and prompt injection attacks. The results were encouraging – Magentic-UI either sought user clarification or outright blocked the unsafe actions in those cases, rather than blindly executing them. This demonstrates that the system’s safeguards and human-in-the-loop checks can effectively mitigate many potential risks. Magentic-UI also runs by default with no persistent login credentials in the browser, ensuring that the agent cannot accidentally leverage the user’s authenticated sessions on websites. All these technical improvements (sandboxing, allow-lists, approval gates, etc.) are aimed at making the agent **transparent, secure, and controllable** in real-world use,.\n",
       "\n",
       "## Performance and Notable Findings  \n",
       "Over the past year, Microsoft has evaluated Magentic-UI’s performance both as an autonomous agent and with human-in-the-loop assistance. The underlying multi-agent system already proved capable on complex tasks: **Magentic-One** (without user input) demonstrated competitive results on several benchmarks of agent performance【33:1†source】. For example, it achieved strong success rates on the **GAIA** challenge (a test of general AI assistants requiring reasoning, tool use, and web interaction) and on **WebArena/WebGames** (interactive web browsing tasks), coming close to prior state-of-the-art systems【33:3†source】,. These results were attained with Magentic-One’s generalist architecture *without* needing specialized training or tuning for each task, highlighting the power of the modular multi-agent approach【33:1†source】.\n",
       "\n",
       "More interestingly, Magentic-UI’s human-centered features have led to **significant improvements** in task completion when even minimal user guidance is provided. In an experiment on the GAIA benchmark, allowing the agent to ask for occasional help or confirmation from a (simulated) user boosted the task success rate from about **30.3% to 51.9%**, a relative increase of **71%** in completion rate. In other words, by inserting a bit of human feedback into the loop, the agent solved many more tasks correctly than it could autonomously. Notably, Magentic-UI achieved this boost with only **light user involvement** – the system needed to request help in only roughly **10%** of the tasks, averaging just **1.1** user interactions per task in that experiment. This suggests that a small amount of timely human oversight can have a outsized positive impact on an agent’s reliability. As one report put it, *“in some scenarios, task completion went up by 71% just by including light human feedback”*【33:5†source】. These findings validate Magentic-UI’s design philosophy: rather than pursuing full autonomy at any cost, keeping a human in the loop for guidance and error correction dramatically improves outcomes on complex tasks while still maintaining efficiency.\n",
       "\n",
       "Another notable benefit observed is the impact of **plan reuse and learning** on efficiency. As mentioned, Magentic-UI can save successful workflows and retrieve them later. In tests, this **plan gallery** mechanism reduced the time to complete repeat tasks by up to threefold, since the agent could skip recalculating steps and immediately deploy a proven plan. For users, this means the agent becomes faster and more effective with each similar task it tackles – a form of cumulative learning. Additionally, Magentic-UI performed well even with **different AI models** powering it. The system is model-agnostic; while the default uses a version of GPT-4 for the Orchestrator, experiments have shown it can incorporate other language models to balance cost and performance【33:2†source】. This flexibility implies researchers or developers can plug in newer or domain-specific models into the agent roles to further improve certain capabilities (for instance, using a specialized vision model for the WebSurfer’s image analysis or a smaller, faster model for the Coder if real-time response is crucial)【33:2†source】.\n",
       "\n",
       "Crucially, the introduction of Magentic-UI did not reveal major new failure modes beyond those already present in agentic systems, and the added oversight actually helps catch issues. The transparent interface allowed testers to catch and correct agent mistakes on the fly, preventing the kind of hidden errors that fully-autonomous agents might carry through unnoticed. The internal evaluations, including the red-team tests described earlier, indicate that Magentic-UI’s layered safety approach is effective: the system **passed phishing and prompt-injection tests** by either refusing the malicious action or deferring to the user. This is a promising result for the safe deployment of such agents. Of course, Magentic-UI is not yet infallible – there were still tasks it could not complete and errors it could not overcome – but the overall trend shows that combining AI automation with human oversight yields a more robust and trustworthy performance.\n",
       "\n",
       "## Conclusion and Outlook  \n",
       "Magentic-UI represents a significant step forward in the evolution of “agentic” AI systems, demonstrating how **human-AI collaboration** can improve both the effectiveness and safety of AI assistants. By keeping the user in the loop, the system addresses long-standing concerns about transparency and loss of control in autonomous AI. Rather than aiming to replace the user, Magentic-UI is designed to **empower** the user – handling the busywork of web-based tasks while continually soliciting user guidance for critical decisions. Early research indicates that this approach can substantially boost task success rates (as seen with the 71% improvement on GAIA) without imposing a heavy burden on the user,. The open-source release of Magentic-UI (and its underlying Magentic-One framework) is also notable – it invites the broader research and developer community to experiment with the system, reproduce results, and build upon its human-centered design【33:5†source】,. Microsoft’s team has highlighted that Magentic-UI is still a prototype and **far from human-level performance** on many tasks, and there are open challenges to tackle around reliability and potential misuse【33:1†source】. As AI agents become more powerful, ensuring they remain **helpful, aligned, and safe** will be crucial【33:1†source】. Magentic-UI provides a valuable platform to explore these questions in a grounded way. \n",
       "\n",
       "In summary, over the past year Microsoft’s AutoGen project has evolved from the multi-agent capabilities of Magentic-One to the human-centric interface of Magentic-UI – adding transparency, user feedback loops, and safety features to create a more **trustworthy AI assistant for the web**. This research prototype has demonstrated encouraging results in combining the strengths of AI and human problem-solving. It lays a strong foundation for future intelligent assistants that can handle complex tasks *with* us rather than for us, ensuring that we as users maintain clarity and control. The work on Magentic-UI not only showcases new features and technical improvements in agent design, but also offers insights and tools (like AutoGenBench and the open-source codebase) for the wider AI community to advance the state of the art in cooperative, human-aligned AI systems. The coming years will likely build on these ideas, bringing us closer to AI agents that are both highly capable and **deeply collaborative** with their human partners. \n",
       "\n",
       "**Sources:** The information in this report is derived from Microsoft Research publications, technical documentation, and reputable AI industry analyses. Key references include the Microsoft Research blog announcements of *Magentic-One* (Nov 2024) and *Magentic-UI* (May 2025)【33:1†source】,, the Magentic-One technical report on arXiv【33:3†source】, the official Magentic-UI GitHub documentation,, and summaries by trusted tech outlets such as MarkTechPost and BetaNews that covered Magentic-UI’s features and experimental results,【33:5†source】. These sources provide detailed accounts of the system’s design, capabilities, and the latest research findings supporting the above summary.\n",
       "\n",
       "## References\n",
       "- [Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks ...](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)\n",
       "- [Microsoft AI Introduces Magentic-UI: An Open-Source Agent Prototype ...](https://www.marktechpost.com/2025/05/22/microsoft-ai-introduces-magentic-ui-an-open-source-agent-prototype-that-works-with-people-to-complete-complex-tasks-that-require-multi-step-planning-and-browser-use/)\n",
       "- [Microsoft Magentic-UI is an open source AI tool that lets humans stay ...](https://betanews.com/2025/05/19/microsoft-magentic-ui-is-an-open-source-ai-tool-that-lets-humans-stay-in-control/)\n",
       "- [Using Magentic-UI, Microsoft’s Open-Source Human-centric web agent](https://www.thewindowsclub.com/use-microsofts-open-source-human-centric-web-agent-magentic-ui)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(md_results_file, 'r', encoding='utf-8') as file:\n",
    "    markdown_content = file.read()\n",
    "    display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debed3a4-0986-4335-9cc5-0351b2cc0654",
   "metadata": {},
   "source": [
    "### Exporting results into a docx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0f21b63-8749-459e-b7c1-e6640336206f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No pandoc was found: either install pandoc and add it\nto your PATH or or call pypandoc.download_pandoc(...) or\ninstall pypandoc wheels with included pandoc.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpypandoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmd_results_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdocx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocx_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmd_results_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has been converted to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocx_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mls -lh $docx_file\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/AZURE AI AGENT SERVICE/.venv2/lib/python3.13/site-packages/pypandoc/__init__.py:206\u001b[39m, in \u001b[36mconvert_file\u001b[39m\u001b[34m(source_file, to, format, extra_args, encoding, outputfile, filters, verify_format, sandbox, cworkdir, sort_files)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(discovered_source_files) == \u001b[32m1\u001b[39m:\n\u001b[32m    204\u001b[39m     discovered_source_files = discovered_source_files[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscovered_source_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m                  \u001b[49m\u001b[43moutputfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mverify_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msandbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43msandbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mcworkdir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcworkdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/AZURE AI AGENT SERVICE/.venv2/lib/python3.13/site-packages/pypandoc/__init__.py:367\u001b[39m, in \u001b[36m_convert_input\u001b[39m\u001b[34m(source, format, input_type, to, extra_args, outputfile, filters, verify_format, sandbox, cworkdir, sort_files)\u001b[39m\n\u001b[32m    364\u001b[39m _check_log_handler()\n\u001b[32m    366\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mEnsuring pandoc path...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[43m_ensure_pandoc_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_format:\n\u001b[32m    370\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mVerifying format...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/AZURE AI AGENT SERVICE/.venv2/lib/python3.13/site-packages/pypandoc/__init__.py:802\u001b[39m, in \u001b[36m_ensure_pandoc_path\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    794\u001b[39m logger.info(textwrap.dedent(\u001b[33m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m    795\u001b[39m \u001b[33m    See http://johnmacfarlane.net/pandoc/installing.html\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[33m    for installation options\u001b[39m\n\u001b[32m    797\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m))\n\u001b[32m    798\u001b[39m logger.info(textwrap.dedent(\u001b[33m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m    799\u001b[39m \u001b[33m    ---------------------------------------------------------------\u001b[39m\n\u001b[32m    800\u001b[39m \n\u001b[32m    801\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo pandoc was found: either install pandoc and add it\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    803\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mto your PATH or or call pypandoc.download_pandoc(...) or\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    804\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33minstall pypandoc wheels with included pandoc.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: No pandoc was found: either install pandoc and add it\nto your PATH or or call pypandoc.download_pandoc(...) or\ninstall pypandoc wheels with included pandoc."
     ]
    }
   ],
   "source": [
    "pypandoc.convert_file(md_results_file, 'docx', outputfile=docx_file)\n",
    "print(f\"{md_results_file} has been converted to {docx_file}\\n\")\n",
    "\n",
    "!ls -lh $docx_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73423e-69d0-4bd5-a6bc-8758670ad536",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = FileLink(path=docx_file)\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b9b8f-4c43-4994-bfbc-ad2b6e15702d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
